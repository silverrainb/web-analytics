{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5 Part 2 Assignment:\n",
    "## CUNY MSDS DATA620 - Web Analytics\n",
    "---\n",
    "### Team5: Christopher Estevez, Meaghan Burke, Rickidon Singh,  Ritesh Lohiya, Rose Koh\n",
    "### 07/16/2018 (due date)\n",
    "##### python version: 2.7\n",
    "---\n",
    "\n",
    "## Document Classification\n",
    "\n",
    "\n",
    "It can be useful to be able to classify new \"test\" documents using already classified \"training\" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  Here is one example of such data:  http://archive.ics.uci.edu/ml/datasets/Spambase\n",
    "\n",
    "For this project, you can either use the above dataset to predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).\n",
    "\n",
    "For more adventurous students, you are welcome (encouraged!) to come up a different set of documents (including scraped web pages!?) that have already been classified (e.g. tagged), then analyze these documents to predict how new documents should be classified.\n",
    "\n",
    "This assignment is due end of day on Monday, July 16th.  You may work in a small team if you want.\n",
    "\n",
    "\n",
    "## Dataset Source and Description\n",
    "The team used the dataset from http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names\n",
    "Header names:  http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names\n",
    "Data without headers: http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
    "\n",
    "The last column of 'spambase.data' denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail. Most of the attributes indicate whether a particular word or character was frequently occuring in the e-mail. The run-length attributes (55-57) measure the length of sequences of consecutive capital letters. For the statistical measures of each attribute, see the end of this file. Here are the definitions of the attributes:\n",
    "\n",
    "48 continuous real [0,100] attributes of type word_freq_WORD\n",
    "= percentage of words in the e-mail that match WORD, i.e. 100 * (number of times the WORD appears in the e-mail) / total number of words in e-mail. A \"word\" in this case is any string of alphanumeric characters bounded by non-alphanumeric characters or end-of-string.\n",
    "\n",
    "6 continuous real [0,100] attributes of type char_freq_CHAR]\n",
    "= percentage of characters in the e-mail that match CHAR, i.e. 100 * (number of CHAR occurences) / total characters in e-mail\n",
    "\n",
    "1 continuous real [1,...] attribute of type capital_run_length_average\n",
    "= average length of uninterrupted sequences of capital letters\n",
    "\n",
    "1 continuous integer [1,...] attribute of type capital_run_length_longest\n",
    "= length of longest uninterrupted sequence of capital letters\n",
    "\n",
    "1 continuous integer [1,...] attribute of type capital_run_length_total\n",
    "= sum of length of uninterrupted sequences of capital letters\n",
    "= total number of capital letters in the e-mail\n",
    "\n",
    "1 nominal {0,1} class attribute of type spam\n",
    "= denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "import sklearn.metrics as sm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\",\n",
    "       \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n",
    "       \"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\",\n",
    "       \"word_freq_credit\", \"word_freq_your\", \"word_freq_font\", \"word_freq_000\",\n",
    "       \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\",\n",
    "       \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\", \"word_freq_telnet\",\n",
    "       \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\",\n",
    "       \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\",\n",
    "       \"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\",\n",
    "       \"word_freq_project\", \"word_freq_re\", \"word_freq_edu\", \"word_freq_table\",\n",
    "       \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\", \"char_freq_[\",\n",
    "       \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \"capital_run_length_average\",\n",
    "       \"capital_run_length_longest\", \"capital_run_length_total\", \"spam\"\n",
    "       ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "First, Let's look at our data summary statistics. Next, we want to create and test a model on this dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>word_freq_receive</th>\n",
       "      <th>word_freq_will</th>\n",
       "      <th>word_freq_people</th>\n",
       "      <th>word_freq_report</th>\n",
       "      <th>word_freq_addresses</th>\n",
       "      <th>word_freq_free</th>\n",
       "      <th>word_freq_business</th>\n",
       "      <th>word_freq_email</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>0.059824</td>\n",
       "      <td>0.541702</td>\n",
       "      <td>0.093930</td>\n",
       "      <td>0.058626</td>\n",
       "      <td>0.049205</td>\n",
       "      <td>0.248848</td>\n",
       "      <td>0.142586</td>\n",
       "      <td>0.184745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>0.201545</td>\n",
       "      <td>0.861698</td>\n",
       "      <td>0.301036</td>\n",
       "      <td>0.335184</td>\n",
       "      <td>0.258843</td>\n",
       "      <td>0.825792</td>\n",
       "      <td>0.444055</td>\n",
       "      <td>0.531122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>2.610000</td>\n",
       "      <td>9.670000</td>\n",
       "      <td>5.550000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.410000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>7.140000</td>\n",
       "      <td>9.090000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq_order  word_freq_mail  word_freq_receive  word_freq_will  \\\n",
       "count      4601.000000     4601.000000        4601.000000     4601.000000   \n",
       "mean          0.090067        0.239413           0.059824        0.541702   \n",
       "std           0.278616        0.644755           0.201545        0.861698   \n",
       "min           0.000000        0.000000           0.000000        0.000000   \n",
       "25%           0.000000        0.000000           0.000000        0.000000   \n",
       "50%           0.000000        0.000000           0.000000        0.100000   \n",
       "75%           0.000000        0.160000           0.000000        0.800000   \n",
       "max           5.260000       18.180000           2.610000        9.670000   \n",
       "\n",
       "       word_freq_people  word_freq_report  word_freq_addresses  \\\n",
       "count       4601.000000       4601.000000          4601.000000   \n",
       "mean           0.093930          0.058626             0.049205   \n",
       "std            0.301036          0.335184             0.258843   \n",
       "min            0.000000          0.000000             0.000000   \n",
       "25%            0.000000          0.000000             0.000000   \n",
       "50%            0.000000          0.000000             0.000000   \n",
       "75%            0.000000          0.000000             0.000000   \n",
       "max            5.550000         10.000000             4.410000   \n",
       "\n",
       "       word_freq_free  word_freq_business  word_freq_email     ...       \\\n",
       "count     4601.000000         4601.000000      4601.000000     ...        \n",
       "mean         0.248848            0.142586         0.184745     ...        \n",
       "std          0.825792            0.444055         0.531122     ...        \n",
       "min          0.000000            0.000000         0.000000     ...        \n",
       "25%          0.000000            0.000000         0.000000     ...        \n",
       "50%          0.000000            0.000000         0.000000     ...        \n",
       "75%          0.100000            0.000000         0.000000     ...        \n",
       "max         20.000000            7.140000         9.090000     ...        \n",
       "\n",
       "       char_freq_;  char_freq_(  char_freq_[  char_freq_!  char_freq_$  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.038575     0.139030     0.016976     0.269071     0.075811   \n",
       "std       0.243471     0.270355     0.109394     0.815672     0.245882   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.065000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.188000     0.000000     0.315000     0.052000   \n",
       "max       4.385000     9.752000     4.081000    32.478000     6.003000   \n",
       "\n",
       "       char_freq_#  capital_run_length_average  capital_run_length_longest  \\\n",
       "count  4601.000000                 4601.000000                 4601.000000   \n",
       "mean      0.044238                    5.191515                   52.172789   \n",
       "std       0.429342                   31.729449                  194.891310   \n",
       "min       0.000000                    1.000000                    1.000000   \n",
       "25%       0.000000                    1.588000                    6.000000   \n",
       "50%       0.000000                    2.276000                   15.000000   \n",
       "75%       0.000000                    3.706000                   43.000000   \n",
       "max      19.829000                 1102.500000                 9989.000000   \n",
       "\n",
       "       capital_run_length_total         spam  \n",
       "count               4601.000000  4601.000000  \n",
       "mean                 283.289285     0.394045  \n",
       "std                  606.347851     0.488698  \n",
       "min                    1.000000     0.000000  \n",
       "25%                   35.000000     0.000000  \n",
       "50%                   95.000000     0.000000  \n",
       "75%                  266.000000     1.000000  \n",
       "max                15841.000000     1.000000  \n",
       "\n",
       "[8 rows x 50 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data = pd.read_csv(\"https://raw.githubusercontent.com/silverrainb/web-analytics/master/week5/spambase.csv\", names = names)\n",
    "spam_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be able to get a good result having a total of 4601 cases that are madeup of 1813 spam cases and 2788 nonspam cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1813\n",
      "2788\n"
     ]
    }
   ],
   "source": [
    "count_spam = len(spam_data[spam_data.spam==1])\n",
    "count_nonspam = len(spam_data[spam_data.spam==0])\n",
    "print(count_spam) \n",
    "print(count_nonspam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling the data into train test and validation sets\n",
    "sampling = np.random.choice((\"train\", \"test\", \"val\"), p=[0.7, 0.15, 0.15], size=len(spam_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3237\n",
      "713\n",
      "651\n",
      "4601\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set, val_set =  spam_data[sampling == \"train\"], spam_data[sampling == \"test\"],spam_data[sampling == \"val\"]\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "print(len(val_set))\n",
    "print(len(train_set)+len(test_set)+len(val_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k Nearest Neighbors\n",
    "Let's check the accuracy using the nearest neighbor algorithm. Using spam as the outcome variable and calling it train_truth, the remaining variables will be train_predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "train_truth = train_set.iloc[:,-1:].values.ravel()\n",
    "train_predictors = train_set.iloc[:,:-1]\n",
    "knn.fit(train_predictors, train_truth)\n",
    "knn_prediction = knn.predict(train_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1744</td>\n",
       "      <td>218</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>239</td>\n",
       "      <td>1036</td>\n",
       "      <td>1275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1983</td>\n",
       "      <td>1254</td>\n",
       "      <td>3237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0     1   All\n",
       "True                       \n",
       "0          1744   218  1962\n",
       "1           239  1036  1275\n",
       "All        1983  1254  3237"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = pd.crosstab(train_truth, knn_prediction, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.89      0.88      1962\n",
      "          1       0.83      0.81      0.82      1275\n",
      "\n",
      "avg / total       0.86      0.86      0.86      3237\n",
      "\n",
      "\n",
      "Report:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.87947554, 0.8261563 ]),\n",
       " array([0.88888889, 0.81254902]),\n",
       " array([0.88415716, 0.81929616]),\n",
       " array([1962, 1275]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print \"\\nClassification Report: \\n\\n\" + str(classification_report(train_truth, knn_prediction)) \n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "print \"\\nReport:\" \n",
    "precision_recall_fscore_support(train_truth, knn_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "def custom_accuracy (report):\n",
    "    ppv_non_spam = report[0][0] # precision = first row -- so [0], non-spam = first col, so [0]\n",
    "    ppv_spam = report[0][1]\n",
    "    return ((2*ppv_non_spam) + ppv_spam)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.861702461352115"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_accuracy = custom_accuracy(precision_recall_fscore_support(train_truth, knn_prediction))\n",
    "knn_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "86% is good, but let's try to improve on it using other methods such as Random Forest and SVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build confusion matrix function\n",
    "def cm_metrics(Actual, Predicted):\n",
    "   cm = sm.confusion_matrix(Actual, Predicted, labels=[1, 0])\n",
    "   print(\"True positives: %d\" %cm[0,0])\n",
    "   print(\"False positives: %d\" %cm[1,0])\n",
    "   print(\"True negatives: %d\" %cm[1,1])\n",
    "   print(\"False negatives: %d\" %cm[0,1])\n",
    "   print\n",
    "   print(sm.classification_report(Actual, Predicted, labels=[1,0], target_names=[\"Spam\", \"Ham\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 1272\n",
      "False positives: 4\n",
      "True negatives: 1958\n",
      "False negatives: 3\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Spam       1.00      1.00      1.00      1275\n",
      "        Ham       1.00      1.00      1.00      1962\n",
      "\n",
      "avg / total       1.00      1.00      1.00      3237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier using Random Forest method\n",
    "train_tar = train_set['spam']\n",
    "train_dat = train_set.drop(labels='spam', axis=1)\n",
    "\n",
    "randon_forest = ensemble.RandomForestClassifier(criterion=\"entropy\", random_state=1)\n",
    "randon_forest_fit = randon_forest.fit(train_dat, train_tar)\n",
    "\n",
    "randon_forest_train = randon_forest_fit.predict(train_dat)\n",
    "cm_metrics(train_tar, randon_forest_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 224\n",
      "False positives: 9\n",
      "True negatives: 400\n",
      "False negatives: 18\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Spam       0.96      0.93      0.94       242\n",
      "        Ham       0.96      0.98      0.97       409\n",
      "\n",
      "avg / total       0.96      0.96      0.96       651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Validation data\n",
    "val_tar = val_set['spam']\n",
    "val_dat = val_set.drop(labels='spam', axis=1)\n",
    "\n",
    "randon_forest_val = randon_forest_fit.predict(val_dat)\n",
    "cm_metrics(val_tar, randon_forest_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 272\n",
      "False positives: 24\n",
      "True negatives: 393\n",
      "False negatives: 24\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Spam       0.92      0.92      0.92       296\n",
      "        Ham       0.94      0.94      0.94       417\n",
      "\n",
      "avg / total       0.93      0.93      0.93       713\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "test_tar = test_set['spam']\n",
    "test_dat = test_set.drop(labels='spam', axis=1)\n",
    "\n",
    "randon_forest_test = randon_forest_fit.predict(test_dat)\n",
    "cm_metrics(test_tar, randon_forest_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 1135\n",
      "False positives: 50\n",
      "True negatives: 1912\n",
      "False negatives: 140\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Spam       0.96      0.89      0.92      1275\n",
      "        Ham       0.93      0.97      0.95      1962\n",
      "\n",
      "avg / total       0.94      0.94      0.94      3237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lets try SVM\n",
    "train_tar = train_set['spam']\n",
    "train_dat = train_set.drop(labels='spam', axis=1)\n",
    "svm = svm.SVC(random_state=1)\n",
    "svm_fit = svm.fit(train_dat, train_tar)\n",
    "\n",
    "svm_train = svm_fit.predict(train_dat)\n",
    "cm_metrics(train_tar, svm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 197\n",
      "False positives: 69\n",
      "True negatives: 340\n",
      "False negatives: 45\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Spam       0.74      0.81      0.78       242\n",
      "        Ham       0.88      0.83      0.86       409\n",
      "\n",
      "avg / total       0.83      0.82      0.83       651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Validation data\n",
    "val_tar = val_set['spam']\n",
    "val_dat = val_set.drop(labels='spam', axis=1)\n",
    "\n",
    "svm_val = svm_fit.predict(val_dat)\n",
    "cm_metrics(val_tar, svm_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 232\n",
      "False positives: 67\n",
      "True negatives: 350\n",
      "False negatives: 64\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Spam       0.78      0.78      0.78       296\n",
      "        Ham       0.85      0.84      0.84       417\n",
      "\n",
      "avg / total       0.82      0.82      0.82       713\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "test_tar = test_set['spam']\n",
    "\n",
    "test_dat = test_set.drop(labels='spam', axis=1)\n",
    "\n",
    "svm_test = svm_fit.predict(test_dat)\n",
    "cm_metrics(test_tar, svm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "Let's use Decision Tree to see the accuracy by utilizing class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight={0: 2, 1: 1}, criterion='gini',\n",
       "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(class_weight = {0:2,1:1})\n",
    "tree.fit(train_predictors, train_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1962</td>\n",
       "      <td>0</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1273</td>\n",
       "      <td>1275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1964</td>\n",
       "      <td>1273</td>\n",
       "      <td>3237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0     1   All\n",
       "True                       \n",
       "0          1962     0  1962\n",
       "1             2  1273  1275\n",
       "All        1964  1273  3237"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_prediction = tree.predict(train_predictors)\n",
    "pd.crosstab(train_truth, tree_prediction, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993211133740666"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_accuracy = custom_accuracy(precision_recall_fscore_support(train_truth, tree_prediction))\n",
    "tree_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ignore the k nearest neighbor as it isn't at par when compared to the other models used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
