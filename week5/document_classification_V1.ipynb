{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5 Part 2 Assignment:\n",
    "## CUNY MSDS DATA620 - Web Analytics\n",
    "---\n",
    "### Team5: Christopher Estevez, Meaghan Burke, Rickidon Singh,  Ritesh Lohiya, Rose Koh\n",
    "### 07/16/2018 (due date)\n",
    "##### python version: ?\n",
    "---\n",
    "\n",
    "## Document Classification\n",
    "\n",
    "\n",
    "It can be useful to be able to classify new \"test\" documents using already classified \"training\" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  Here is one example of such data:  http://archive.ics.uci.edu/ml/datasets/Spambase\n",
    "\n",
    "For this project, you can either use the above dataset to predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).\n",
    "\n",
    "For more adventurous students, you are welcome (encouraged!) to come up a different set of documents (including scraped web pages!?) that have already been classified (e.g. tagged), then analyze these documents to predict how new documents should be classified.\n",
    "\n",
    "This assignment is due end of day on Monday, July 16th.  You may work in a small team if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "import sklearn.metrics as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\",\n",
    "        \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\", \n",
    "        \"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \n",
    "        \"word_freq_credit\", \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \n",
    "        \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\",\n",
    "        \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\", \"word_freq_telnet\", \n",
    "        \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\", \n",
    "        \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\",\n",
    "        \"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\",\n",
    "        \"word_freq_project\", \"word_freq_re\", \"word_freq_edu\", \"word_freq_table\",\n",
    "        \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\", \"char_freq_[\", \n",
    "        \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \"capital_run_length_average\", \n",
    "        \"capital_run_length_longest\", \"capital_run_length_total\", \"spam\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_data = pd.read_csv(\"https://raw.githubusercontent.com/silverrainb/web-analytics/master/week5/spambase.csv\", names = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>word_freq_receive</th>\n",
       "      <th>word_freq_will</th>\n",
       "      <th>word_freq_people</th>\n",
       "      <th>word_freq_report</th>\n",
       "      <th>word_freq_addresses</th>\n",
       "      <th>word_freq_free</th>\n",
       "      <th>word_freq_business</th>\n",
       "      <th>word_freq_email</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>0.059824</td>\n",
       "      <td>0.541702</td>\n",
       "      <td>0.093930</td>\n",
       "      <td>0.058626</td>\n",
       "      <td>0.049205</td>\n",
       "      <td>0.248848</td>\n",
       "      <td>0.142586</td>\n",
       "      <td>0.184745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>0.201545</td>\n",
       "      <td>0.861698</td>\n",
       "      <td>0.301036</td>\n",
       "      <td>0.335184</td>\n",
       "      <td>0.258843</td>\n",
       "      <td>0.825792</td>\n",
       "      <td>0.444055</td>\n",
       "      <td>0.531122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>2.610000</td>\n",
       "      <td>9.670000</td>\n",
       "      <td>5.550000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.410000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>7.140000</td>\n",
       "      <td>9.090000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq_order  word_freq_mail  word_freq_receive  word_freq_will  \\\n",
       "count      4601.000000     4601.000000        4601.000000     4601.000000   \n",
       "mean          0.090067        0.239413           0.059824        0.541702   \n",
       "std           0.278616        0.644755           0.201545        0.861698   \n",
       "min           0.000000        0.000000           0.000000        0.000000   \n",
       "25%           0.000000        0.000000           0.000000        0.000000   \n",
       "50%           0.000000        0.000000           0.000000        0.100000   \n",
       "75%           0.000000        0.160000           0.000000        0.800000   \n",
       "max           5.260000       18.180000           2.610000        9.670000   \n",
       "\n",
       "       word_freq_people  word_freq_report  word_freq_addresses  \\\n",
       "count       4601.000000       4601.000000          4601.000000   \n",
       "mean           0.093930          0.058626             0.049205   \n",
       "std            0.301036          0.335184             0.258843   \n",
       "min            0.000000          0.000000             0.000000   \n",
       "25%            0.000000          0.000000             0.000000   \n",
       "50%            0.000000          0.000000             0.000000   \n",
       "75%            0.000000          0.000000             0.000000   \n",
       "max            5.550000         10.000000             4.410000   \n",
       "\n",
       "       word_freq_free  word_freq_business  word_freq_email     ...       \\\n",
       "count     4601.000000         4601.000000      4601.000000     ...        \n",
       "mean         0.248848            0.142586         0.184745     ...        \n",
       "std          0.825792            0.444055         0.531122     ...        \n",
       "min          0.000000            0.000000         0.000000     ...        \n",
       "25%          0.000000            0.000000         0.000000     ...        \n",
       "50%          0.000000            0.000000         0.000000     ...        \n",
       "75%          0.100000            0.000000         0.000000     ...        \n",
       "max         20.000000            7.140000         9.090000     ...        \n",
       "\n",
       "       char_freq_;  char_freq_(  char_freq_[  char_freq_!  char_freq_$  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.038575     0.139030     0.016976     0.269071     0.075811   \n",
       "std       0.243471     0.270355     0.109394     0.815672     0.245882   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.065000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.188000     0.000000     0.315000     0.052000   \n",
       "max       4.385000     9.752000     4.081000    32.478000     6.003000   \n",
       "\n",
       "       char_freq_#  capital_run_length_average  capital_run_length_longest  \\\n",
       "count  4601.000000                 4601.000000                 4601.000000   \n",
       "mean      0.044238                    5.191515                   52.172789   \n",
       "std       0.429342                   31.729449                  194.891310   \n",
       "min       0.000000                    1.000000                    1.000000   \n",
       "25%       0.000000                    1.588000                    6.000000   \n",
       "50%       0.000000                    2.276000                   15.000000   \n",
       "75%       0.000000                    3.706000                   43.000000   \n",
       "max      19.829000                 1102.500000                 9989.000000   \n",
       "\n",
       "       capital_run_length_total         spam  \n",
       "count               4601.000000  4601.000000  \n",
       "mean                 283.289285     0.394045  \n",
       "std                  606.347851     0.488698  \n",
       "min                    1.000000     0.000000  \n",
       "25%                   35.000000     0.000000  \n",
       "50%                   95.000000     0.000000  \n",
       "75%                  266.000000     1.000000  \n",
       "max                15841.000000     1.000000  \n",
       "\n",
       "[8 rows x 50 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1813\n",
      "2788\n"
     ]
    }
   ],
   "source": [
    "count_spam = len(spam_data[spam_data.spam==1])\n",
    "count_nonspam = len(spam_data[spam_data.spam==0])\n",
    "print(count_spam) \n",
    "print(count_nonspam) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling the data into train test and validation sets\n",
    "sampling = np.random.choice((\"train\", \"test\", \"val\"), p=[0.7, 0.15, 0.15], size=len(spam_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3171\n",
      "712\n",
      "718\n",
      "4601\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set, val_set =  spam_data[sampling == \"train\"], spam_data[sampling == \"test\"],spam_data[sampling == \"val\"]\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "print(len(val_set))\n",
    "print(len(train_set)+len(test_set)+len(val_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build confusion matrix function\n",
    "def cm_metrics(Actual, Predicted):\n",
    "    cm = sm.confusion_matrix(Actual, Predicted, labels=[1, 0])\n",
    "    print(\"True positives: %d\" %cm[0,0])\n",
    "    print(\"False positives: %d\" %cm[1,0])\n",
    "    print(\"True negatives: %d\" %cm[1,1])\n",
    "    print(\"False negatives: %d\" %cm[0,1])\n",
    "    print\n",
    "    print(sm.classification_report(Actual, Predicted, labels=[1,0], target_names=[\"Spam\", \"Ham\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will use Randon Forest Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 1228\n",
      "False positives: 1\n",
      "True negatives: 1927\n",
      "False negatives: 15\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Spam       1.00      0.99      0.99      1243\n",
      "        Ham       0.99      1.00      1.00      1928\n",
      "\n",
      "avg / total       0.99      0.99      0.99      3171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier usinf Random Forest method\n",
    "train_tar = train_set['spam']\n",
    "train_dat = train_set.drop(labels='spam', axis=1)\n",
    "\n",
    "randon_forest = ensemble.RandomForestClassifier(criterion=\"entropy\", random_state=1)\n",
    "randon_forest_fit = randon_forest.fit(train_dat, train_tar)\n",
    "\n",
    "randon_forest_train = randon_forest_fit.predict(train_dat)\n",
    "cm_metrics(train_tar, randon_forest_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 261\n",
      "False positives: 19\n",
      "True negatives: 416\n",
      "False negatives: 22\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Spam       0.93      0.92      0.93       283\n",
      "        Ham       0.95      0.96      0.95       435\n",
      "\n",
      "avg / total       0.94      0.94      0.94       718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Validation data\n",
    "val_tar = val_set['spam']\n",
    "val_dat = val_set.drop(labels='spam', axis=1)\n",
    "\n",
    "randon_forest_val = randon_forest_fit.predict(val_dat)\n",
    "cm_metrics(val_tar, randon_forest_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 259\n",
      "False positives: 16\n",
      "True negatives: 409\n",
      "False negatives: 28\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Spam       0.94      0.90      0.92       287\n",
      "        Ham       0.94      0.96      0.95       425\n",
      "\n",
      "avg / total       0.94      0.94      0.94       712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "test_tar = test_set['spam']\n",
    "test_dat = test_set.drop(labels='spam', axis=1)\n",
    "\n",
    "randon_forest_test = randon_forest_fit.predict(test_dat)\n",
    "cm_metrics(test_tar, randon_forest_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9382022471910112"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy = (TP+FN)/(TP+TN+FP+FN)\n",
    "Accuarcy = (259+409)/(259+16+409+28)\n",
    "Accuarcy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 1121\n",
      "False positives: 53\n",
      "True negatives: 1875\n",
      "False negatives: 122\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Spam       0.95      0.90      0.93      1243\n",
      "        Ham       0.94      0.97      0.96      1928\n",
      "\n",
      "avg / total       0.95      0.94      0.94      3171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lets try SVM\n",
    "train_tar = train_set['spam']\n",
    "train_dat = train_set.drop(labels='spam', axis=1)\n",
    "svm = svm.SVC(random_state=1)\n",
    "svm_fit = svm.fit(train_dat, train_tar)\n",
    "\n",
    "svm_train = svm_fit.predict(train_dat)\n",
    "cm_metrics(train_tar, svm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 232\n",
      "False positives: 82\n",
      "True negatives: 353\n",
      "False negatives: 51\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Spam       0.74      0.82      0.78       283\n",
      "        Ham       0.87      0.81      0.84       435\n",
      "\n",
      "avg / total       0.82      0.81      0.82       718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Validation data\n",
    "val_tar = val_set['spam']\n",
    "val_dat = val_set.drop(labels='spam', axis=1)\n",
    "\n",
    "svm_val = svm_fit.predict(val_dat)\n",
    "cm_metrics(val_tar, svm_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 219\n",
      "False positives: 66\n",
      "True negatives: 359\n",
      "False negatives: 68\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Spam       0.77      0.76      0.77       287\n",
      "        Ham       0.84      0.84      0.84       425\n",
      "\n",
      "avg / total       0.81      0.81      0.81       712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "test_tar = test_set['spam']\n",
    "test_dat = test_set.drop(labels='spam', axis=1)\n",
    "\n",
    "svm_test = svm_fit.predict(test_dat)\n",
    "cm_metrics(test_tar, svm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8117977528089888"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy = (TP+FN)/(TP+TN+FP+FN)\n",
    "Accuarcy = (219+359)/(219+66+359+68)\n",
    "Accuarcy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try now try to classify using Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 1241\n",
      "False positives: 0\n",
      "True negatives: 1928\n",
      "False negatives: 2\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Spam       1.00      1.00      1.00      1243\n",
      "        Ham       1.00      1.00      1.00      1928\n",
      "\n",
      "avg / total       1.00      1.00      1.00      3171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lets us see the accuracy using Decision Tree\n",
    "train_tar = train_set['spam']\n",
    "train_dat = train_set.drop(labels='spam', axis=1)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(class_weight = {0:2,1:1})\n",
    "tree_fit = tree.fit(train_dat, train_tar)\n",
    "\n",
    "tree_train = tree_fit.predict(train_dat)\n",
    "cm_metrics(train_tar, tree_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 253\n",
      "False positives: 30\n",
      "True negatives: 405\n",
      "False negatives: 30\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Spam       0.89      0.89      0.89       283\n",
      "        Ham       0.93      0.93      0.93       435\n",
      "\n",
      "avg / total       0.92      0.92      0.92       718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Validation data\n",
    "val_tar = val_set['spam']\n",
    "val_dat = val_set.drop(labels='spam', axis=1)\n",
    "\n",
    "tree_val = tree_fit.predict(val_dat)\n",
    "cm_metrics(val_tar, tree_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 257\n",
      "False positives: 38\n",
      "True negatives: 387\n",
      "False negatives: 30\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Spam       0.87      0.90      0.88       287\n",
      "        Ham       0.93      0.91      0.92       425\n",
      "\n",
      "avg / total       0.91      0.90      0.90       712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "test_tar = test_set['spam']\n",
    "test_dat = test_set.drop(labels='spam', axis=1)\n",
    "\n",
    "tree_test = tree_fit.predict(test_dat)\n",
    "cm_metrics(test_tar, tree_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9044943820224719"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy = (TP+FN)/(TP+TN+FP+FN)\n",
    "Accuarcy = (257+387)/(257+38+387+30)\n",
    "Accuarcy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "Out of the three methods we tried Random Forest has the best accuracy rate(94%) compared to SVM("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
